# extract_jobs.py

import requests
from bs4 import BeautifulSoup
import json
import time
import csv
from cleaning import *

BASE_URL = "https://www.cake.me"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; StudentBot/1.0; +https://ncu.edu.tw)"
}

def scrape_jobs(pages=5):
    jobs = []

    for page in range(1, pages + 1):  # é è¨­æŠ“å–å‰ N é 
        print(f"ğŸ“„ æ­£åœ¨æ“·å–ç¬¬ {page} é è³‡æ–™...")
        list_url = f"{BASE_URL}/jobs?order=latest&page={page}"
        
        response = requests.get(list_url, headers=HEADERS)
        soup = BeautifulSoup(response.text, "html.parser")
        job_cards = soup.select("div.JobSearchItem_container__oKoBL")

        for card in job_cards:
            try:
                title_tag = card.select_one("a[class^='JobSearchItem_jobTitle__']")
                company_tag = card.select_one("a[class^='JobSearchItem_companyName__']")
                location_tags = card.select("a[class^='JobSearchItem_featureSegmentLink__']")

                job_url = BASE_URL + title_tag["href"] if title_tag else ""
                requirements = ""

                if job_url:
                    detail_resp = requests.get(job_url, headers=HEADERS)
                    detail_soup = BeautifulSoup(detail_resp.text, "html.parser")

                    section_blocks = detail_soup.select("div.ContentSection_contentSection__ELRlG")
                    for section in section_blocks:
                        h3 = section.select_one("h3.ContentSection_title__fcZYs")
                        if h3 and "è·å‹™éœ€æ±‚" in h3.text:
                            content_div = section.select_one("div.ContentSection_content__e3ios")
                            if content_div:
                                requirements = content_div.get_text(separator="\n", strip=True)
                            break

                job = {
                    "title": clean_text(title_tag.text) if title_tag else "",
                    "url": job_url,
                    "company": clean_text(company_tag.text) if company_tag else "",
                    "location": normalize_list([tag.text for tag in location_tags]),
                    "requirements": clean_html(str(content_div)) if 'content_div' in locals() and content_div else ""
                }

                jobs.append(job)
                time.sleep(1)

            except Exception as e:
                print("âŒ æ“·å–å¤±æ•—ï¼š", e)

    # å„²å­˜ç‚º JSON
    with open("cake_jobs_pages.json", "w", encoding="utf-8") as f:
        json.dump(jobs, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²æ“·å– {len(jobs)} ç­†è·ç¼ºè³‡æ–™ä¸¦å„²å­˜ç‚º JSON")

    return jobs  # å›å‚³è³‡æ–™ä¾›å…¶ä»–æ¨¡çµ„ä½¿ç”¨ï¼ˆå¯é¸ï¼‰

# âœ… å¦‚æœç›´æ¥åŸ·è¡Œ extract_jobs.pyï¼ˆè€Œä¸æ˜¯ importï¼‰ï¼Œå°±åŸ·è¡Œçˆ¬èŸ²
if __name__ == "__main__":
    scrape_jobs()
